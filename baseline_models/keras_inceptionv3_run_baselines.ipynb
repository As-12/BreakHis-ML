{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP-CW4 Baseline execution notebook\n",
    "\n",
    "This notebook is not for data exploration. But to log execution of experiments.\n",
    "This job runs baseline 5 times and save results to files/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as12production/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import matplotlib\n",
    "import keras\n",
    "# import the necessary packages\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import shutil\n",
    "import fnmatch\n",
    "import pickle\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input\n",
    "from keras.constraints import max_norm\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras.initializers\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "import keras.optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras setting\n",
    "K.set_image_dim_ordering('tf')  #Image dimension ordering\n",
    "\n",
    "#Random seeds\n",
    "SEED = 2082018\n",
    "np.random.seed(SEED)\n",
    "\n",
    "#Image Settings\n",
    "IMAGE_RESIZE = (224,224)  # Image size. Process on the by data generator. Should match CNN model input. \n",
    "IMAGE_INPUT_SIZE = (224,224,3) #Image Input size to the neural network\n",
    "\n",
    "#Training Settings\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 20\n",
    "\n",
    "#Directories\n",
    "# NOTE: The labels are determined by subfolders. PNG or JPEG images only.\n",
    "TRAIN_DIR = '../TRAIN' \n",
    "VAL_DIR = '../VALID'\n",
    "TEST_DIR =  '../TEST'\n",
    "\n",
    "#Index of the class label represents numerical representation\n",
    "CLASS_LABELS = [\"Benign\", \"Malignant\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildImageDataset(path, imageResize=None,shuffle=False,seed=0):\n",
    "    \"\"\"\n",
    "    Load dataset into an array. Labels are defined by folder name.\n",
    "    \"\"\"\n",
    "    filenames = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    imagePaths = sorted(list(paths.list_images(path)))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(imagePaths)\n",
    "\n",
    "    for imagePath in imagePaths:\n",
    "        image = cv2.imread(imagePath)\n",
    "        if imageResize != None:\n",
    "            image = cv2.resize(image, imageResize)\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "        filenames.append(imagePath)\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        labels.append(CLASS_LABELS.index(label))\n",
    "    return (np.array(data), np.array(labels), np.array(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_this = 0\n",
    "\n",
    "if run_this == 1:\n",
    "    X_train, y_train, _ = buildImageDataset(TRAIN_DIR,imageResize=IMAGE_RESIZE,seed=SEED)\n",
    "    X_valid, y_valid, _ = buildImageDataset(VAL_DIR,imageResize=IMAGE_RESIZE,seed=SEED)\n",
    "\n",
    "    with open(\"X_train.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(X_train, output_file)\n",
    "\n",
    "    with open(\"y_train.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(y_train, output_file)\n",
    "\n",
    "    with open(\"X_valid.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(X_valid, output_file)\n",
    "\n",
    "    with open(\"y_valid.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(y_valid, output_file)\n",
    "else:\n",
    "    X_train = pickle.load( open( \"X_train.pickle\", \"rb\" ) )\n",
    "    y_train = pickle.load( open( \"y_train.pickle\", \"rb\" ) )\n",
    "    X_valid = pickle.load( open( \"X_valid.pickle\", \"rb\" ) )\n",
    "    y_valid = pickle.load( open( \"y_valid.pickle\", \"rb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255.0\n",
    "X_valid /= 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_valid = keras.utils.to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Building Models\n",
    "def InceptionNet():\n",
    "    #https://keras.io/applications/#inceptionv3\n",
    "    #Use Inception 3 without the last layer.\n",
    "    #Replace last layer with 1 sigmoid for binary classification\n",
    "    sgd = SGD(lr=0.01, momentum=0.9,nesterov=False)\n",
    "    model = keras.applications.inception_v3.InceptionV3(include_top=False,\n",
    "                                                        weights='imagenet',  #Pre-train on ImageNet \n",
    "                                                        input_tensor=Input(shape=(224,224,3)),\n",
    "                                                        input_shape=None,\n",
    "                                                        pooling='avg',\n",
    "                                                        classes=2)\n",
    "    final = Model(input=model.input,output=Dense(2, activation='softmax')(model.output))\n",
    "    final.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    plot = plt.figure()\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True)\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrainingPlots(history, accuracy_grap_file, loss_graph_file):\n",
    "    plt.style.use('ggplot')\n",
    "    accuracy_plot = plt.figure(figsize=(15,10))\n",
    "    for k in ['val_acc', 'acc']:\n",
    "        data = np.array(history[k])\n",
    "        plt.plot(data)\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['acc(valid)', 'acc(train)'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    loss_plot = plt.figure(figsize=(15,10))\n",
    "    for k in ['loss', 'val_loss']:\n",
    "        data = np.array(history[k])\n",
    "        plt.plot(data)\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Error (Log Loss)')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['error(train)', 'error(valid)'], loc='upper left')\n",
    "    plt.show()\n",
    "    #Save visualization data    \n",
    "    accuracy_plot.savefig(accuracy_grap_file, bbox_inches='tight')\n",
    "    loss_plot.savefig(loss_graph_file, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateClassifier(history, classifier, weights, X ,y , cm_file, out_file, classlabels):\n",
    "    classifier.load_weights(weights)\n",
    "    y_pred = classifier.predict(X)\n",
    "    cm = confusion_matrix(np.argmax(y,axis=1), np.argmax(np.rint(y_pred),axis=1))\n",
    "    cm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot = plot_confusion_matrix(cm_norm, classes=classlabels)\n",
    "    plot.savefig(cm_file, bbox_inches='tight')\n",
    "    scores = classifier.evaluate(X_valid,y_valid)\n",
    "    with open(out_file, 'w') as the_file:\n",
    "        the_file.write(\"Max Val Acc: \" + str(np.max(np.array(history['val_acc']))))\n",
    "        the_file.write(\"\\nMin Val Acc: \" + str(np.min(np.array(history['val_acc']))))\n",
    "        the_file.write(\"\\nMax Train Acc: \"  + str(np.max(np.array(history['acc']))))\n",
    "        the_file.write(\"\\nMin Train Acc: \"  + str(np.min(np.array(history['acc']))))\n",
    "        the_file.write(\"\\nMax Val Err: \"  + str(np.max(np.array(history['val_loss']))))\n",
    "        the_file.write(\"\\nMin Val Err: \" + str(np.min(np.array(history['val_loss']))))\n",
    "        the_file.write(\"\\nMax Train Err: \" + str(np.max(np.array(history['loss']))))\n",
    "        the_file.write(\"\\nMin Train Err: \" +  str(np.min(np.array(history['loss']))))\n",
    "        the_file.write(\"\\nResult: \" + str(scores))\n",
    "        the_file.write(\"\\nResult: \" +  str(model.metrics_names))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Visualizing & Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as12production/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5252 samples, validate on 1735 samples\n",
      "Epoch 1/20\n",
      "5252/5252 [==============================] - 138s 26ms/step - loss: 0.2944 - acc: 0.8713 - val_loss: 0.8485 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79078, saving model to run#1_baseline-inception3-weights.hdf5\n",
      "Epoch 2/20\n",
      "5252/5252 [==============================] - 130s 25ms/step - loss: 0.1365 - acc: 0.9480 - val_loss: 0.7932 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/20\n",
      "5252/5252 [==============================] - 130s 25ms/step - loss: 0.0981 - acc: 0.9667 - val_loss: 0.5915 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.79078 to 0.85014, saving model to run#1_baseline-inception3-weights.hdf5\n",
      "Epoch 4/20\n",
      "5252/5252 [==============================] - 130s 25ms/step - loss: 0.0442 - acc: 0.9844 - val_loss: 2.6745 - val_acc: 0.5879\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/20\n",
      "5252/5252 [==============================] - 130s 25ms/step - loss: 0.0421 - acc: 0.9869 - val_loss: 1.9210 - val_acc: 0.7107\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/20\n",
      "5252/5252 [==============================] - 130s 25ms/step - loss: 0.0807 - acc: 0.9741 - val_loss: 0.7719 - val_acc: 0.7948\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/20\n",
      "5252/5252 [==============================] - 130s 25ms/step - loss: 0.0973 - acc: 0.9634 - val_loss: 1.4594 - val_acc: 0.7303\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/20\n",
      "2272/5252 [===========>..................] - ETA: 1:06 - loss: 0.0256 - acc: 0.9908"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(\"Running Experiment Iteration\",i)\n",
    "    model = InceptionNet()\n",
    "    #Process filenames\n",
    "    prefix = \"run#{}_\".format(i)\n",
    "    MODEL_CHECKPOINT_FILE=prefix + \"baseline-inception3-weights.hdf5\"  \n",
    "    #Training charts and graphics\n",
    "    MODEL_TRAIN_RESULTS_FILE=prefix + \"baseline-inception3_train.pickle\"\n",
    "    MODEL_ACCURACY_GRAPH_FILE=prefix + \"baseline-inception3_accuracy.pdf\"\n",
    "    MODEL_LOSS_GRAPH_FILE=prefix + \"baseline-inception3_loss.pdf\"\n",
    "    MODEL_EVALUATION_CM_FILE=prefix + \"baseline-inception3_CM.pdf\"\n",
    "    MODEL_EVALUATION_TEXT_FILE=prefix + \"baseline-inception3_eval.txt\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_FILE, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    #Training\n",
    "    history = model.fit(x=X_train,y=y_train,\n",
    "                        validation_data=(X_valid,y_valid),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs = EPOCH,\n",
    "                        callbacks = callbacks_list)\n",
    "    \n",
    "    #Saving training result\n",
    "    with open(MODEL_TRAIN_RESULTS_FILE, \"wb\") as output_file:\n",
    "        pickle.dump(history.history, output_file)    \n",
    "    \n",
    "    #Generate training graphs\n",
    "    generateTrainingPlots(history.history, MODEL_ACCURACY_GRAPH_FILE, MODEL_LOSS_GRAPH_FILE)\n",
    "    \n",
    "    #evaluate classifier\n",
    "    evaluateClassifier(history.history, InceptionNet(), MODEL_CHECKPOINT_FILE, \n",
    "                       X_valid ,y_valid , MODEL_EVALUATION_CM_FILE, MODEL_EVALUATION_TEXT_FILE, CLASS_LABELS)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
