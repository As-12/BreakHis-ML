{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP-CW4 Experiment 3 100x  execution notebook\n",
    "\n",
    "This notebook is not for data exploration. But to log execution of experiments.\n",
    "This job runs baseline 5 times and save results to files/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as12production/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import matplotlib\n",
    "import keras\n",
    "# import the necessary packages\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import shutil\n",
    "import fnmatch\n",
    "import pickle\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input\n",
    "from keras.constraints import max_norm\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras.initializers\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "import keras.optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras setting\n",
    "K.set_image_dim_ordering('tf')  #Image dimension ordering\n",
    "\n",
    "#Random seeds\n",
    "SEED = 2082018\n",
    "np.random.seed(SEED)\n",
    "\n",
    "#Image Settings\n",
    "IMAGE_RESIZE = (224,224)  # Image size. Process on the by data generator. Should match CNN model input. \n",
    "IMAGE_INPUT_SIZE = (224,224,3) #Image Input size to the neural network\n",
    "\n",
    "#Training Settings\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 20\n",
    "\n",
    "#Directories\n",
    "# NOTE: The labels are determined by subfolders. PNG or JPEG images only.\n",
    "TRAIN_DIR = '../TRAIN' \n",
    "VAL_DIR = '../VALID'\n",
    "TEST_DIR =  '../TEST'\n",
    "\n",
    "\n",
    "#[\"-40\",\"-100-\",\"-200-\",\"-400-\"]\n",
    "magnifications = \"-100-\"\n",
    "\n",
    "\n",
    "#Index of the class label represents numerical representation\n",
    "CLASS_LABELS = [\"Benign\", \"Malignant\"]\n",
    "NUM_CLASSES = 2\n",
    "#Checkpoints and save files\n",
    "\n",
    "#Saving every epochs that improve val accuracy\n",
    "#MODEL_CHECKPOINT_FILE=\"baseline_model-weights-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# Rewriting save file for epoch that improves val accuracy\n",
    "MODEL_CHECKPOINT_FILE=\"inception3-weights.hdf5\"  \n",
    "\n",
    "#Training charts and graphics\n",
    "MODEL_TRAIN_RESULTS_FILE=\"inception3_train.pickle\"\n",
    "MODEL_ACCURACY_GRAPH_FILE=\"inception3_accuracy.pdf\"\n",
    "MODEL_LOSS_GRAPH_FILE=\"inception3_loss.pdf\"\n",
    "MODEL_EVALUATION_CM_FILE=\"inception3_CM.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildImageDataset(path, imageResize=None,shuffle=False,seed=0):\n",
    "    \"\"\"\n",
    "    Load dataset into an array. Labels are defined by folder name.\n",
    "    \"\"\"\n",
    "    filenames = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    imagePaths = sorted(list(paths.list_images(path)))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(imagePaths)\n",
    "\n",
    "    for imagePath in imagePaths:\n",
    "        image = cv2.imread(imagePath)\n",
    "        if imageResize != None:\n",
    "            image = cv2.resize(image, imageResize)\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "        filenames.append(imagePath)\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        labels.append(CLASS_LABELS.index(label))\n",
    "    return (np.array(data), np.array(labels), np.array(filenames))\n",
    "\n",
    "def showClassDistribution(y, labels):\n",
    "    figure = plt.figure(figsize=(10,5))\n",
    "    ax = sns.countplot(x = y)\n",
    "    ax.set_xticklabels(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_this = 1\n",
    "\n",
    "# Load and save\n",
    "if run_this == 1:\n",
    "    X_train, y_train, train_files = buildImageDataset(TRAIN_DIR,imageResize=IMAGE_RESIZE,seed=SEED)\n",
    "    X_valid, y_valid, valid_files = buildImageDataset(VAL_DIR,imageResize=IMAGE_RESIZE,seed=SEED)\n",
    "    \n",
    "    with open(\"X_train.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(X_train, output_file)\n",
    "\n",
    "    with open(\"y_train.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(y_train, output_file)\n",
    "\n",
    "    with open(\"X_valid.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(X_valid, output_file)\n",
    "\n",
    "    with open(\"y_valid.pickle\", \"wb\") as output_file:\n",
    "            pickle.dump(y_valid, output_file)\n",
    "    \n",
    "            \n",
    "# Quick load from saved files\n",
    "if run_this == 2:\n",
    "    X_train = pickle.load( open( \"X_train.pickle\", \"rb\" ) )\n",
    "    y_train = pickle.load( open( \"y_train.pickle\", \"rb\" ) )\n",
    "    X_valid = pickle.load( open( \"X_valid.pickle\", \"rb\" ) )\n",
    "    y_valid = pickle.load( open( \"y_valid.pickle\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_this = 1\n",
    "\n",
    "if run_this == 1:\n",
    "    validDataFrame = pd.DataFrame({'label':y_valid, 'filename':valid_files})\n",
    "    trainDataFrame = pd.DataFrame({'label':y_train, 'filename':train_files})\n",
    "    \n",
    "    index = [i for i,item in enumerate(trainDataFrame[\"filename\"]) if magnifications in item]\n",
    "    X_train = X_train[index]\n",
    "    y_train = y_train[index]\n",
    "    train_files = train_files[index]\n",
    "    \n",
    "    index = [i for i,item in enumerate(validDataFrame[\"filename\"]) if magnifications in item]\n",
    "    X_valid = X_valid[index]\n",
    "    y_valid = y_valid[index]\n",
    "    valid_files = valid_files[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(X_train[-1])\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.imshow(X_train[1])\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "ax3.imshow(X_valid[-1])\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "ax4.imshow(X_valid[1])\n",
    "\n",
    "fig.savefig(\"100x_rescaled_images.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataFrame = pd.DataFrame({'label':y_train, 'filename':train_files})\n",
    "showClassDistribution(y_train, CLASS_LABELS)\n",
    "print(\"There are {} items in training set.\".format(len(y_train) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validDataFrame = pd.DataFrame({'label':y_valid, 'filename':valid_files})\n",
    "showClassDistribution(y_valid, CLASS_LABELS)\n",
    "print(\"There are {} items in validation set.\".format(len(y_valid) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255.0\n",
    "X_valid /= 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_valid = keras.utils.to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Building Models\n",
    "def InceptionNet():\n",
    "    #https://keras.io/applications/#inceptionv3\n",
    "    #Use Inception 3 without the last layer.\n",
    "    #Replace last layer with 1 sigmoid for binary classification\n",
    "    sgd = SGD(lr=0.01, momentum=0.9,nesterov=False)\n",
    "    model = keras.applications.inception_v3.InceptionV3(include_top=False,\n",
    "                                                        weights='imagenet',  #Pre-train on ImageNet \n",
    "                                                        input_tensor=Input(shape=IMAGE_INPUT_SIZE),\n",
    "                                                        input_shape=None,\n",
    "                                                        pooling='avg',\n",
    "                                                        classes=NUM_CLASSES)\n",
    "    final = Model(input=model.input,output=Dense(NUM_CLASSES, activation='softmax')(model.output))\n",
    "    final.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    plot = plt.figure()\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True)\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrainingPlots(history, accuracy_grap_file, loss_graph_file):\n",
    "    plt.style.use('ggplot')\n",
    "    accuracy_plot = plt.figure(figsize=(15,10))\n",
    "    for k in ['val_acc', 'acc']:\n",
    "        data = np.array(history[k])\n",
    "        plt.plot(data)\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylim((0,1.2))\n",
    "    plt.legend(['acc(valid)', 'acc(train)'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    loss_plot = plt.figure(figsize=(15,10))\n",
    "    for k in ['loss', 'val_loss']:\n",
    "        data = np.array(history[k])\n",
    "        plt.plot(data)\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Error (Log Loss)')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['error(train)', 'error(valid)'], loc='upper left')\n",
    "    plt.show()\n",
    "    #Save visualization data    \n",
    "    accuracy_plot.savefig(accuracy_grap_file, bbox_inches='tight')\n",
    "    loss_plot.savefig(loss_graph_file, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateClassifier(history, classifier, weights, X ,y , cm_file, out_file, classlabels):\n",
    "    classifier.load_weights(weights)\n",
    "    y_pred = classifier.predict(X)\n",
    "    cm = confusion_matrix(np.argmax(y,axis=1), np.argmax(np.rint(y_pred),axis=1))\n",
    "    cm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot = plot_confusion_matrix(cm_norm, classes=classlabels)\n",
    "    plot.savefig(cm_file, bbox_inches='tight')\n",
    "    scores = classifier.evaluate(X_valid,y_valid)\n",
    "    with open(out_file, 'w') as the_file:\n",
    "        the_file.write(\"Max Val Acc: \" + str(np.max(np.array(history['val_acc']))))\n",
    "        the_file.write(\"\\nMin Val Acc: \" + str(np.min(np.array(history['val_acc']))))\n",
    "        the_file.write(\"\\nMax Train Acc: \"  + str(np.max(np.array(history['acc']))))\n",
    "        the_file.write(\"\\nMin Train Acc: \"  + str(np.min(np.array(history['acc']))))\n",
    "        the_file.write(\"\\nMax Val Err: \"  + str(np.max(np.array(history['val_loss']))))\n",
    "        the_file.write(\"\\nMin Val Err: \" + str(np.min(np.array(history['val_loss']))))\n",
    "        the_file.write(\"\\nMax Train Err: \" + str(np.max(np.array(history['loss']))))\n",
    "        the_file.write(\"\\nMin Train Err: \" +  str(np.min(np.array(history['loss']))))\n",
    "        the_file.write(\"\\nResult: \" + str(scores))\n",
    "        the_file.write(\"\\nResult: \" +  str(model.metrics_names))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Visualizing & Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    print(\"Running Experiment Iteration\",i)\n",
    "    model = InceptionNet()\n",
    "    #Process filenames\n",
    "    prefix = \"run#{}_\".format(i)\n",
    "    MODEL_CHECKPOINT_FILE=prefix + \"experiment3\"+magnifications+\"inception3-weights.hdf5\"  \n",
    "    #Training charts and graphics\n",
    "    MODEL_TRAIN_RESULTS_FILE=prefix + \"experiment3\"+magnifications+\"inception3_train.pickle\"\n",
    "    MODEL_ACCURACY_GRAPH_FILE=prefix + \"experiment3\"+magnifications+\"inception3_accuracy.pdf\"\n",
    "    MODEL_LOSS_GRAPH_FILE=prefix + \"experiment3\"+magnifications+\"inception3_loss.pdf\"\n",
    "    MODEL_EVALUATION_CM_FILE=prefix + \"experiment3\"+magnifications+\"inception3_CM.pdf\"\n",
    "    MODEL_EVALUATION_TEXT_FILE=prefix + \"experiment3\"+magnifications+\"inception3_eval.txt\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_FILE, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    #Training\n",
    "    history = model.fit(x=X_train,y=y_train,\n",
    "                        validation_data=(X_valid,y_valid),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs = EPOCH,\n",
    "                        callbacks = callbacks_list)\n",
    "    \n",
    "    #Saving training result\n",
    "    with open(MODEL_TRAIN_RESULTS_FILE, \"wb\") as output_file:\n",
    "        pickle.dump(history.history, output_file)    \n",
    "    \n",
    "    #Generate training graphs\n",
    "    generateTrainingPlots(history.history, MODEL_ACCURACY_GRAPH_FILE, MODEL_LOSS_GRAPH_FILE)\n",
    "    \n",
    "    #evaluate classifier\n",
    "    evaluateClassifier(history.history, InceptionNet(), MODEL_CHECKPOINT_FILE, \n",
    "                       X_valid ,y_valid , MODEL_EVALUATION_CM_FILE, MODEL_EVALUATION_TEXT_FILE, CLASS_LABELS)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
